{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pyspark \n",
    "!pip3 install sklearn \n",
    "!pip3 install nltk\n",
    "!pip3 install matplotlib\n",
    "!pip3 install wordcloud\n",
    "!pip3 install plotly\n",
    "!pip3 install numpy\n",
    "!pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull, when, count, col, avg\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import cluster\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly import offline\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "\n",
    "import math\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup connection parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri_db = 'mongodb+srv://host:SVg9g$x6^EUEjp@cluster0.vyk6g.mongodb.net'\n",
    "spark_connector_uri = 'org.mongodb.spark:mongo-spark-connector_2.11:2.2.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession object.\n",
    "session = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .config('spark.mongodb.input.uri', uri_db) \\\n",
    "    .config('spark.jars.packages', spark_connector_uri) \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", '50000') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get context from SparkSession object.\n",
    "context = session.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available databases and number of rows for each collection in every database.\n",
    "# * test :  28k reviews,   1k meta;\n",
    "# * test2: 118k reviews,  10k meta;\n",
    "# * test3: 616k reviews,  50k meta;\n",
    "# * data : 3.3m reviews, 532k meta.\n",
    "db_name = 'data'\n",
    "\n",
    "# Read data from MongoDB and return two DataFrame objects, one\n",
    "# for each collection contained in database.\n",
    "df_reviews = session.read \\\n",
    "    .format('com.mongodb.spark.sql.DefaultSource') \\\n",
    "    .option('database', db_name) \\\n",
    "    .option('collection', 'reviews') \\\n",
    "    .load()\n",
    "df_meta = session.read \\\n",
    "    .format('com.mongodb.spark.sql.DefaultSource') \\\n",
    "    .option('database', db_name) \\\n",
    "    .option('collection', 'meta') \\\n",
    "    .load()\n",
    "\n",
    "# Print collections schemas.\n",
    "df_reviews.printSchema()\n",
    "df_meta.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop MongoDB _id column.\n",
    "df_reviews = df_reviews.drop('_id')\n",
    "df_meta = df_meta.drop('_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_helpful_rate(array):\n",
    "    '''\n",
    "        Compute the fraction of users that found the review helpful.\n",
    "\n",
    "        Args:\n",
    "            array (array): two elements array.\n",
    "                \n",
    "            E.g. [1, 2], one found the review helpful over a total of two.\n",
    "        Returns:\n",
    "            float: percentage of user that found the review helpful.\n",
    "    '''\n",
    "    num = array[0]\n",
    "    den = array[1]\n",
    "    res = 0\n",
    "\n",
    "    if den != 0:\n",
    "        res = num/den * 100\n",
    "        \n",
    "    return float(res)\n",
    "\n",
    "# Create helpful_rate and helpful_pos columns from helpful column.\n",
    "df_reviews = df_reviews.rdd \\\n",
    "    .map(lambda x: x + (get_helpful_rate(x['helpful']), x['helpful'][0], )) \\\n",
    "    .toDF(df_reviews.columns + ['helpful_rate', 'helpful_pos']) \\\n",
    "    .drop('helpful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sales_rank columns from salesRank column.\n",
    "df_meta = df_meta \\\n",
    "    .join(df_meta.select(['asin', 'salesRank.Sports &amp; Outdoors']), on='asin') \\\n",
    "    .drop('salesRank') \\\n",
    "    .withColumnRenamed('Sports &amp; Outdoors', 'sales_rank')\n",
    "\n",
    "# Drop categories, related and imUrl columns.\n",
    "df_meta = df_meta \\\n",
    "    .drop('categories', 'related', 'imUrl')\n",
    "\n",
    "# Count null values per column.\n",
    "print('(Pre-cleaning) Null values per column: ')\n",
    "df_meta.select([count(when(isnull(c), c)).alias(c) for c in df_meta.columns]).show()\n",
    "\n",
    "# Fill price null values or NaNs with mean value of the price column.\n",
    "df_meta = df_meta.na.fill({'price': df_meta.dropna(subset=['price']).agg(avg('price')).first()[0]})\n",
    "# Fill sales_rank_sports_etc null values with mean value of the sales_rank column.\n",
    "df_meta = df_meta.na.fill({'sales_rank': df_meta.dropna(subset=['sales_rank']).agg(avg('sales_rank')).first()[0]})\n",
    "# Fill brand null values or empty strings with 'No Brand'.\n",
    "df_meta = df_meta \\\n",
    "    .withColumn('brand', when(col('brand') != '', col('brand')).otherwise(None))\n",
    "df_meta = df_meta.na.fill({'brand': 'No Brand'})\n",
    "# Fill title empty strings with 'No Title'\n",
    "df_meta = df_meta \\\n",
    "    .withColumn('title', when(col('title') != '', col('title')).otherwise(None))\n",
    "df_meta = df_meta.na.fill({'title': 'No Title'})\n",
    "# Fill description empty strings with 'No Description'.\n",
    "df_meta = df_meta \\\n",
    "    .withColumn('description', when(col('description') != '', col('description')).otherwise(None))\n",
    "df_meta = df_meta.na.fill({'description': 'No Description'})\n",
    "\n",
    "print('(Post-cleaning) Null values per column: ')\n",
    "df_meta.select([count(when(isnull(c), c)).alias(c) for c in df_meta.columns]).show()\n",
    "\n",
    "brand_categorical = {}\n",
    "index_categorical = 1\n",
    "def map_brand_to_categorical(brand):\n",
    "    '''\n",
    "        Map brand's name to categorical.\n",
    "\n",
    "        Args:\n",
    "            brand (string): brand's name.\n",
    "        Returns:\n",
    "            int: categorical value.\n",
    "    '''\n",
    "    global brand_categorical\n",
    "    global index_categorical\n",
    "\n",
    "    if brand not in brand_categorical.keys():\n",
    "        brand_categorical[brand] = index_categorical\n",
    "        index_categorical += 1\n",
    "\n",
    "    return brand_categorical[brand]\n",
    "\n",
    "# Map brand's name to categorical.\n",
    "brands = df_meta.select('brand').rdd.map(lambda x: x[0]).collect()\n",
    "cat = [map_brand_to_categorical(brand) for brand in brands]\n",
    "\n",
    "df_meta = df_meta.rdd \\\n",
    "    .map(lambda x: x + (brand_categorical[x['brand']], )) \\\n",
    "    .toDF(df_meta.columns + ['brand_cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query #1\n",
    "df_1 = df_reviews \\\n",
    "    .groupby('asin') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'reviews_count_product') \\\n",
    "    .orderBy('reviews_count_product', ascending=False)\n",
    "\n",
    "print(\"# I 100 prodotti con il maggior numero di recensioni #\")\n",
    "df_1.show(100)\n",
    "\n",
    "#df_1.limit(100).coalesce(1).write.format('com.databricks.spark.csv').save('./csv/df_1.csv', header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query #2\n",
    "df_2 = df_reviews \\\n",
    "    .groupby('reviewerID') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'reviews_count_reviewer') \\\n",
    "    .orderBy('reviews_count_reviewer', ascending=False)\n",
    "\n",
    "print(\"# I 100 reviewer che hanno effettuato il maggior numero di recensioni #\")\n",
    "df_2.show(100)\n",
    "\n",
    "#df_2.limit(100).coalesce(1).write.format('com.databricks.spark.csv').save('./csv/df_2.csv', header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query #3\n",
    "df_3 = df_reviews \\\n",
    "    .join(df_meta, on='asin') \\\n",
    "    .filter(\"brand != 'No Brand'\") \\\n",
    "    .groupBy('brand') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'reviews_count_brand') \\\n",
    "    .orderBy('reviews_count_brand', ascending=False)\n",
    "\n",
    "print(\"# Le 50 marche i cui prodotti sono stati maggiormente recensiti #\")\n",
    "df_3.show(50)\n",
    "\n",
    "#df_3.limit(50).coalesce(1).write.format('com.databricks.spark.csv').save('./csv/df_3.csv', header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query #4\n",
    "df_4 =  df_reviews \\\n",
    "     .join(df_meta, on='asin') \\\n",
    "     .select(['brand', 'price']) \\\n",
    "     .filter(\"brand != 'No Brand'\") \\\n",
    "     .groupby('brand') \\\n",
    "     .mean() \\\n",
    "     .withColumnRenamed('avg(price)', 'price_mean') \\\n",
    "     .orderBy('price_mean', ascending=False)\n",
    "\n",
    "print(\"# Le 50 marche i cui prodotti hanno un prezzo medio maggiore #\")\n",
    "df_4.show(50)\n",
    "\n",
    "#df_4.limit(50).coalesce(1).write.format('com.databricks.spark.csv').save('./csv/df_4.csv', header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Query #5\n",
    "df_5 = df_reviews \\\n",
    "    .select(['asin', 'overall']) \\\n",
    "    .groupby('asin') \\\n",
    "    .mean() \\\n",
    "    .withColumnRenamed('avg(overall)', 'overall_mean_product')\n",
    "\n",
    "# Join to df_1 in order to obtain reviews_count_product column.\n",
    "df_5 = df_5 \\\n",
    "    .join(df_1, on='asin') \\\n",
    "    .orderBy(['overall_mean_product', 'reviews_count_product'], ascending=False)\n",
    "\n",
    "print(\"# I 100 prodotti con le migliori recensioni #\")\n",
    "df_5.show(100)\n",
    "\n",
    "#df_5.limit(100).coalesce(1).write.format('com.databricks.spark.csv').save('./csv/df_5.csv', header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query #6\n",
    "df_6 = df_reviews \\\n",
    "    .join(df_meta, on='asin') \\\n",
    "    .select(['brand', 'overall']) \\\n",
    "    .filter(\"brand != 'No Brand'\") \\\n",
    "    .groupBy('brand') \\\n",
    "    .mean() \\\n",
    "    .withColumnRenamed('avg(overall)', 'overall_mean_brand')\n",
    "\n",
    "# Join to df_3 in order to obtain reviews_count_brand column.\n",
    "df_6 = df_6 \\\n",
    "    .join(df_3, on='brand') \\\n",
    "    .orderBy(['overall_mean_brand', 'reviews_count_brand'], ascending=False)\n",
    "\n",
    "print(\"# Le 100 marche con le migliori recensioni #\")\n",
    "df_6.show(100)\n",
    "\n",
    "#df_6.limit(100).coalesce(1).write.format('com.databricks.spark.csv').save('./csv/df_6.csv', header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query #7\n",
    "df_mean = df_reviews \\\n",
    "    .select(['reviewerID', 'helpful_rate']) \\\n",
    "    .groupBy('reviewerID') \\\n",
    "    .mean('helpful_rate') \\\n",
    "    .withColumnRenamed('avg(helpful_rate)', 'helpful_rate_mean')\n",
    "    \n",
    "df_sum = df_reviews \\\n",
    "    .select(['reviewerID', 'helpful_pos']) \\\n",
    "    .filter('helpful_pos != 0') \\\n",
    "    .groupBy('reviewerID') \\\n",
    "    .sum() \\\n",
    "    .withColumnRenamed('sum(helpful_pos)', 'helpful_pos_sum')\n",
    "\n",
    "df_7 = df_mean \\\n",
    "    .join(df_sum, on='reviewerID') \\\n",
    "    .orderBy(['helpful_rate_mean', 'helpful_pos_sum'], ascending=False)\n",
    "    \n",
    "print(\"# I 100 reviewer che hanno effettuato recensioni con la maggiore utilità media #\")\n",
    "df_7.show(100)\n",
    "\n",
    "#df_7.limit(100).coalesce(1).write.format('com.databricks.spark.csv').save('./csv/df_7.csv', header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query #8\n",
    "df_8 = df_7 \\\n",
    "    .orderBy(['helpful_rate_mean', 'helpful_pos_sum'], ascending=True)\n",
    "    \n",
    "print(\"# I 100 reviewer che hanno effettuato recensioni con la minore utilità media #\")\n",
    "df_8.show(100)\n",
    "\n",
    "#df_8.limit(100).coalesce(1).write.format('com.databricks.spark.csv').save('./csv/df_8.csv', header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query #9\n",
    "df_9 = df_meta \\\n",
    "    .orderBy('sales_rank', ascending=True) \\\n",
    "    .select(['asin', 'sales_rank'])\n",
    "\n",
    "print('# I 100 prodotti con il migliore ranking nelle vendite #')\n",
    "df_9.show(100)\n",
    "\n",
    "#df_9.limit(100).coalesce(1).write.format('com.databricks.spark.csv').save('./csv/df_9.csv', header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query #10\n",
    "df_10 = df_meta \\\n",
    "    .select(['brand', 'sales_rank']) \\\n",
    "    .filter(\"brand != 'No Brand'\") \\\n",
    "    .groupby('brand') \\\n",
    "    .mean() \\\n",
    "    .withColumnRenamed('avg(sales_rank)', 'sales_rank_mean') \\\n",
    "    .orderBy(['sales_rank_mean'], ascending=True)\n",
    "\n",
    "print('# Le 50 marche i cui prodotti hanno il ranking medio migliore #')\n",
    "df_10.show(50)\n",
    "\n",
    "#df_10.limit(50).coalesce(1).write.format('com.databricks.spark.csv').save('./csv/df_10.csv', header = 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_sub = df_reviews \\\n",
    "    .drop('reviewText', 'summary', 'reviewTime', 'reviewerName')\n",
    "\n",
    "df_meta_sub = df_meta \\\n",
    "    .drop('brand', 'description', 'title')\n",
    "\n",
    "df_result = df_reviews_sub \\\n",
    "    .join(df_meta_sub, on='asin') \\\n",
    "    .join(df_1, on='asin') \\\n",
    "    .join(df_5.drop('reviews_count_product'), on='asin') \\\n",
    "    .drop('asin', 'reviewerID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "g = sns.pairplot(df_result.toPandas())\n",
    "\n",
    "g.savefig('./figs/pairplot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr_coef(df, first_c, second_c):\n",
    "    '''\n",
    "        Compute correlation coeff between two columns of the same DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): DataFrame object;\n",
    "            first_c (string): name of the first column;\n",
    "            second_c (string): name of the second column.\n",
    "            \n",
    "            N.B. The content of the columns passed must be numeric.\n",
    "        Returns:\n",
    "            float: computed corr coeff.\n",
    "    '''\n",
    "    mat = np.array(df.select(first_c, second_c).collect()).astype(np.float)\n",
    "    return np.corrcoef(mat[:, 0], mat[:, 1])[0, 1]\n",
    "\n",
    "# [PERFORMANCE] Compute single coeffs only if db name is 'test' or 'test2'; otherwhise, compute correlation matrix on next cell.\n",
    "if db_name in ['test', 'test2']:\n",
    "    print('# Correlazione tra il prezzo di un prodotto e il punteggio medio ottenuto nelle recensioni #')\n",
    "    cc1 = get_corr_coef(df_result, 'price', 'overall_mean_product')\n",
    "    print(cc1)\n",
    "\n",
    "    print('# Correlazione tra la marca di un prodotto e il punteggio medio ottenuto nelle recensioni #')\n",
    "    cc2 = get_corr_coef(df_result, 'brand_cat', 'overall_mean_product')\n",
    "    print(cc2)\n",
    "\n",
    "    print(\"# Correlazione tra l'utilità di una recensione e il punteggio assegnato dalla recensione al prodotto #\")\n",
    "    cc3 = get_corr_coef(df_result, 'helpful_rate', 'overall')\n",
    "    print(cc3)\n",
    "\n",
    "    print('# Correlazione tra la data di una recensione e l’utilità della stessa #')\n",
    "    cc4 = get_corr_coef(df_result, 'unixReviewTime', 'helpful_rate')\n",
    "    print(cc4)\n",
    "\n",
    "    print('# Correlazione tra la data di una recensione e il punteggio assegnato al prodotto #')\n",
    "    cc5 = get_corr_coef(df_result, 'unixReviewTime', 'overall')\n",
    "    print(cc5)\n",
    "\n",
    "    print('# Correlazione tra il ranking delle vendite di un prodotto e i punteggi ottenuti nelle recensioni #')\n",
    "    cc6 = get_corr_coef(df_result, 'sales_rank', 'overall')\n",
    "    print(cc6)\n",
    "\n",
    "    print('# Correlazione tra il numero delle recensioni di un prodotto e il ranking nelle vendite #')\n",
    "    cc7 = get_corr_coef(df_result, 'reviews_count_product', 'sales_rank')\n",
    "    print(cc7)\n",
    "\n",
    "    print('# Correlazione tra il ranking nelle vendite e il prezzo #')\n",
    "    cc8 = get_corr_coef(df_result, 'sales_rank', 'price')\n",
    "    print(cc8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Vector column type.\n",
    "assembler = VectorAssembler(inputCols=df_result.columns, outputCol='corr')\n",
    "df_vector = assembler.transform(df_result).select('corr')\n",
    "\n",
    "# Extract labels from DataFrame object.\n",
    "labels = [label for label in df_result.columns]\n",
    "num_labels = len(labels)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "indx = 1\n",
    "for method in ['pearson', 'spearman']:\n",
    "    plt.subplot(1, 2, indx)\n",
    "    \n",
    "    # Compute correlation matrix.\n",
    "    matrix = Correlation.corr(df_vector, 'corr', method)\n",
    "    # Format and reshape correlation matrix.\n",
    "    corr_mat = matrix.collect()[0]['{0}({1})'.format(method, 'corr')].values.reshape(num_labels, num_labels)\n",
    "    \n",
    "    # Set color map.\n",
    "    if method == 'pearson': cmap = 'viridis'\n",
    "    else: cmap = 'magma'\n",
    "    \n",
    "    im = plt.imshow(corr_mat, cmap=cmap)\n",
    "    \n",
    "    # Create annotations with 2 decimals precision.\n",
    "    for y in range(corr_mat.shape[0]):\n",
    "        for x in range(corr_mat.shape[1]):\n",
    "            if x == y or corr_mat[y, x] > 0.6: color_text = 'k'\n",
    "            else: color_text = 'w'\n",
    "            \n",
    "            plt.text(x, y, '%.2f' % corr_mat[y, x], \\\n",
    "                     horizontalalignment='center' , \\\n",
    "                     verticalalignment='center'   , \\\n",
    "                     color=color_text)\n",
    "    \n",
    "    plt.title('{0} method'.format(method.capitalize()), fontsize=16)\n",
    "    # Set axes labels and colorbar.\n",
    "    plt.xticks(range(num_labels), labels, fontsize=14, rotation=90)\n",
    "    plt.yticks(range(num_labels), labels, fontsize=14)\n",
    "    cb = plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    plt.clim(-1, 1)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    indx += 1\n",
    "    \n",
    "plt.savefig('./figs/correlation.png')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Raccommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first 100 objects with most reviews.\n",
    "df_most_reviews = df_meta \\\n",
    "    .join(df_1.limit(100), on='asin')\n",
    "\n",
    "# Count the number of good reviews and bad reviews per object.\n",
    "# N.B. A review is considered good if its overall is greater \n",
    "# than 3; bad if its overall is lesser than 3; neutral otherwise. \n",
    "# Neutral reviews are deliberately ignored in this analysis.\n",
    "df_good_reviews = df_reviews \\\n",
    "    .select('asin', 'overall') \\\n",
    "    .filter('overall > 3') \\\n",
    "    .groupBy('asin') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'good_reviews')\n",
    "\n",
    "df_bad_reviews = df_reviews \\\n",
    "    .select('asin', 'overall') \\\n",
    "    .filter('overall < 3') \\\n",
    "    .groupBy('asin') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'bad_reviews')\n",
    "    \n",
    "# Create column <revenue>, that aggregate <price> and <sales_rank> columns with \n",
    "# the following formula: <revenue> = <price> * exp((1 - <rank>) / mean_rank).\n",
    "mean_rank = df_most_reviews.agg({'sales_rank': 'avg'}).first()[0]\n",
    "df_most_reviews = df_most_reviews.rdd \\\n",
    "    .map(lambda x: x + (x['price'] * math.exp((1 - x['sales_rank']) / mean_rank), )) \\\n",
    "    .toDF(df_most_reviews.columns + ['revenue'])\n",
    "\n",
    "df_most_reviews = df_most_reviews.select('asin', 'revenue')\n",
    "\n",
    "df_result = df_most_reviews \\\n",
    "    .join(df_good_reviews, on='asin') \\\n",
    "    .join(df_bad_reviews, on='asin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_result.collect()\n",
    "X = np.asarray(X).reshape(df_result.count(), len(df_result.columns))\n",
    "\n",
    "# products_infos = [<asin>]\n",
    "pruducts_infos = X[:, 0]\n",
    "# features = [<revenue>, <good_reviews>, <bad_reviews>df_metares = X[:, 1:].astype(np.float)\n",
    "\n",
    "# Normalize features in range [0; 1].\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(features)\n",
    "features_norm = scaler.transform(features)\n",
    "\n",
    "# N.B. KMeans results are not reproducible unless the keyword random_state is not defined.\n",
    "kmeans = cluster.KMeans(5, random_state=10).fit(features_norm)\n",
    "\n",
    "# Get cluster's indices.\n",
    "labels = kmeans.labels_\n",
    "# Map cluster's indices to colors.\n",
    "list_colors = ['#636EFA', '#EF553B', '#00CC96', '#AB63FA', '#FFA15A', '#19D3F3', '#FF6692', '#B6E880', '#FF97FF', '#FECB52']\n",
    "colors = [list_colors[label] for label in labels]\n",
    "\n",
    "fig = px.scatter_3d(x=features_norm[:, 1], y=features_norm[:, 2], z=features_norm[:, 0], color=colors)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Scatter Plot',\n",
    "    scene=dict(\n",
    "        xaxis=dict(\n",
    "            title_text='# Good Reviews (overall > 3)'\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title_text='# Bad  Reviews (overall < 3)'\n",
    "        ),\n",
    "        zaxis=dict(\n",
    "            title_text='Revenue'\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "offline.plot(fig, filename='./figs/3d-scatter.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for cluster color.\n",
    "asins_filtered = [X[i, 0] for i in range(len(colors)) if colors[i] == '#EF553B']\n",
    "\n",
    "# Get title and description attributes with filtered asins.\n",
    "df_filtered = df_meta \\\n",
    "    .select('asin', 'title', 'description') \\\n",
    "    .filter(df_meta['asin'].isin(asins_filtered))\n",
    "titles_descriptions_filtered = np.asarray(df_filtered.collect()).reshape(df_filtered.count(), len(df_filtered.columns))\n",
    "# Merge title and description attributes in a unique text blob.\n",
    "blob = ' '.join([' '.join(a[1: ]) for a in titles_descriptions_filtered])\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Get stopwords array from nltk.\n",
    "stop_words = stopwords.words('english')\n",
    "# Include 'No Brand' e 'No Description' in stopwords.\n",
    "stop_words.append('No')\n",
    "stop_words.append('Brand')\n",
    "stop_words.append('Description')\n",
    "\n",
    "# Remove HTML or XML tags.\n",
    "text_cleaned = re.sub('<[^<]+?>', '', blob)\n",
    "\n",
    "# Remove punctuation.\n",
    "text_no_punctuation = text_cleaned.translate(str.maketrans(' ', ' ', string.punctuation))\n",
    "\n",
    "# Split text.\n",
    "text_splitted = text_no_punctuation.split()\n",
    "\n",
    "# Remove stopwords.\n",
    "text_no_stopwords = [word for word in text_splitted if word not in stop_words]\n",
    "\n",
    "# Identify splitted words as nouns, verbs, etc.\n",
    "text_pos_tag = nltk.pos_tag(text_no_stopwords)\n",
    "    \n",
    "text_filtered = []\n",
    "for pair in text_pos_tag:\n",
    "    word = pair[0]\n",
    "    tag = pair[1]\n",
    "    \n",
    "    # Take only nouns and proper nouns (singular and plural).\n",
    "    if tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        text_filtered.append(word)\n",
    "\n",
    "# Get 100 most frequent words.\n",
    "fdist = FreqDist(text_filtered)\n",
    "fdist.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wordcloud.\n",
    "wordcloud = WordCloud(width=1000, height=1000, background_color='white', stopwords=stop_words, min_font_size=10).generate(' '.join(text_filtered))\n",
    "\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find useful features of chosen product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all Victorinox items with asin equal to B00004YVAJ.\n",
    "df_vict = df_reviews \\\n",
    "    .join(df_meta, on='asin') \\\n",
    "    .filter(\"asin = 'B00004YVAJ'\") \\\n",
    "    .select('title', 'description')\n",
    "\n",
    "# Create text blob with title and description attributes.\n",
    "titles_descriptions_vict = np.asarray(df_vict.collect()).reshape(df_vict.count(), len(df_vict.columns))\n",
    "blob = ' '.join([' '.join(a) for a in titles_descriptions_vict])\n",
    "\n",
    "# Remove HTML or XML tags.\n",
    "text_cleaned = re.sub('<[^<]+?>', '', blob)\n",
    "\n",
    "# Remove punctuation.\n",
    "text_no_punctuation = text_cleaned.translate(str.maketrans(' ', ' ', string.punctuation))\n",
    "\n",
    "# Split text.\n",
    "text_splitted = text_no_punctuation.split()\n",
    "\n",
    "# Remove stopwords.\n",
    "text_no_stopwords = [word for word in text_splitted if word not in stop_words]\n",
    "\n",
    "# Identify splitted words as nouns, verbs, etc.\n",
    "text_pos_tag = nltk.pos_tag(text_no_stopwords)\n",
    "    \n",
    "text_filtered = []\n",
    "for pair in text_pos_tag:\n",
    "    word = pair[0]\n",
    "    tag = pair[1]\n",
    "    \n",
    "    # Take only nouns (singular and plural) and adjectives.\n",
    "    if tag in ['NN', 'NNS', 'JJ', 'JJS', 'JJR']:\n",
    "        text_filtered.append(word)\n",
    "\n",
    "# Get 100 most frequent words.\n",
    "fdist = FreqDist(text_filtered)\n",
    "fdist.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find issues and missing features of the chosen product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary and reviewText attributes from Victorinox items with asin equal\n",
    "# to B00004YVAJ. Also, select negative reviews and reviews with number of people \n",
    "# that found the review useful greater than 0.\n",
    "df_vict = df_reviews \\\n",
    "    .join(df_meta, on='asin') \\\n",
    "    .filter(\"asin = 'B00004YVAJ' and helpful_pos > 0 and overall < 3\") \\\n",
    "    .select('summary', 'reviewText')\n",
    "\n",
    "text = np.asarray(df_vict.collect()).reshape(df_vict.count(), 2)\n",
    "blob = ' '.join([' '.join(a) for a in text])\n",
    "\n",
    "# Remove HTML or XML tags.\n",
    "text_cleaned = re.sub('<[^<]+?>', '', blob)\n",
    "\n",
    "# Remove punctuation.\n",
    "text_no_punctuation = text_cleaned.translate(str.maketrans(' ', ' ', string.punctuation))\n",
    "\n",
    "# Split text.\n",
    "text_splitted = text_no_punctuation.split()\n",
    "\n",
    "# Remove stopwords.\n",
    "text_no_stopwords = [word for word in text_splitted if word not in stop_words]\n",
    "\n",
    "# Identify splitted words as nouns, verbs, etc.\n",
    "text_pos_tag = nltk.pos_tag(text_no_stopwords)\n",
    "\n",
    "text_filtered = []\n",
    "for pair in text_pos_tag:\n",
    "    word = pair[0]\n",
    "    tag = pair[1]\n",
    "    \n",
    "    # Take only nouns (singular and plural) and adjectives.\n",
    "    if tag in ['NN', 'NNS', 'JJ', 'JJS', 'JJR']:\n",
    "        text_filtered.append(word)\n",
    "\n",
    "# Get most common 5-grams in text blob.\n",
    "grams = nltk.ngrams(text_no_stopwords, 5)\n",
    "for g in grams: print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbasecondad54da413211c4fdb9c2b9bc2046d2ac1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
