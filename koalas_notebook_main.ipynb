{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import databricks.koalas as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri_db = 'mongodb+srv://<username>:<password>@bigdata.toqh2.mongodb.net'\n",
    "spark_connector_uri = 'org.mongodb.spark:mongo-spark-connector_2.11:2.2.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession object.\n",
    "session = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .config('spark.mongodb.input.uri', uri_db) \\\n",
    "    .config('spark.jars.packages', spark_connector_uri) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get context from SparkSession object.\n",
    "context = session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from MongoDB and return two DataFrame objects, one\n",
    "# for each collection contained in database.\n",
    "df_reviews = session.read \\\n",
    "    .format('com.mongodb.spark.sql.DefaultSource') \\\n",
    "    .option('database', 'test') \\\n",
    "    .option('collection', 'reviews') \\\n",
    "    .load()\n",
    "df_meta = session.read \\\n",
    "    .format('com.mongodb.spark.sql.DefaultSource') \\\n",
    "    .option('database', 'test') \\\n",
    "    .option('collection', 'meta') \\\n",
    "    .load()\n",
    "\n",
    "# Drop MongoDB _id column in order to avoid error at runtime.\n",
    "df_reviews = df_reviews.drop('_id')\n",
    "df_meta = df_meta.drop('_id')\n",
    "\n",
    "# Print collections schemas.\n",
    "df_reviews.printSchema()\n",
    "df_meta.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two Koalas DataFrame from the Spark DataFrame objects.\n",
    "kdf_reviews = ks.DataFrame(df_reviews)\n",
    "kdf_meta = ks.DataFrame(df_meta)\n",
    "\n",
    "# Extract sports and outdoors data from salesRank struct.\n",
    "array_ranks = df_meta.select('salesRank.Sports &amp; Outdoors').to_koalas()\n",
    "\n",
    "# Allow merge from different DataFrame objects.\n",
    "ks.set_option('compute.ops_on_diff_frames', True)\n",
    "\n",
    "# Assign a new column with the array_ranks data extracted above.\n",
    "kdf_meta['sales_rank_sports_etc'] = array_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1\n",
    "kdf_1 = kdf_reviews \\\n",
    "    .groupby('asin') \\\n",
    "    .size() \\\n",
    "    .sort_values(ascending=False) \\\n",
    "    .head(100)\n",
    "\n",
    "print(\"# I 100 prodotti con il maggior numero di recensioni #\")\n",
    "kdf_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kdf_2 = kdf_reviews \\\n",
    "    .groupby('reviewerID') \\\n",
    "    .size() \\\n",
    "    .sort_values(ascending=False) \\\n",
    "    .head(100)\n",
    "\n",
    "print(\"# I 100 reviewer che hanno effettuato il maggior numero di recensioni #\")\n",
    "kdf_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbasecondad54da413211c4fdb9c2b9bc2046d2ac1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}